{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547d7b1d",
   "metadata": {},
   "source": [
    "# OG Solar Datasets — Cleaning Notebook\n",
    "**What this notebook does**\n",
    "1) Discover original city files; 2) Inspect null %; 3) Label-encode `Assumed_building_type` (stable mapping);\n",
    "4) Drop null rows (after showing percentages first); 5) Remove 0 and negative `Estimated_building_height`;\n",
    "6) Save per-city cleaned Parquet and one combined Parquet.\n",
    "\n",
    "> Run each cell top-to-bottom. If your files are in another folder or Excel/CSV mix, edit the `DATA_DIR` or `FILE_GLOB` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 1: Setup & discovery ===\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Adjust if needed\n",
    "DATA_DIR = Path(\"data/raw\")\n",
    "FILE_GLOB = \"*rooftop*solar*\"\n",
    "\n",
    "# Will save here\n",
    "OUT_DIR = Path(\"data/interim/cleaned\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Supported file types\n",
    "files = sorted(list(DATA_DIR.glob(FILE_GLOB)))\n",
    "files = [p for p in files if p.suffix.lower() in {\".csv\", \".xlsx\", \".xls\"}]\n",
    "\n",
    "print(f\"Found {len(files)} files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "# Common columns we will enforce\n",
    "common_columns = [\n",
    "    \"City\",\n",
    "    \"Surface_area\",\n",
    "    \"Potential_installable_area\",\n",
    "    \"Peak_installable_capacity\",\n",
    "    \"Energy_potential_per_year\",\n",
    "    \"Assumed_building_type\",\n",
    "    \"Estimated_tilt\",\n",
    "    \"Estimated_building_height\",\n",
    "    \"Estimated_capacity_factor\"\n",
    "]\n",
    "\n",
    "# Stable label mapping (text -> int) for Assumed_building_type\n",
    "BT_STR2INT = {\n",
    "    \"commercial\": 0,\n",
    "    \"industrial\": 1,\n",
    "    \"multifamily residential\": 2,\n",
    "    \"public\": 3,\n",
    "    \"single family residential\": 4,\n",
    "}\n",
    "BT_INT2STR = {v:k for k,v in BT_STR2INT.items()}\n",
    "\n",
    "# Helper to read any supported file and standardize columns\n",
    "def read_standardize(path: Path, common_cols):\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    # standardize col names\n",
    "    df.columns = (df.columns\n",
    "                    .str.strip()\n",
    "                    .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    # if City missing or blank, infer from filename stem\n",
    "    if \"City\" not in df.columns:\n",
    "        df[\"City\"] = path.stem.replace(\"_rooftop_solar_potential\", \"\")\n",
    "    # keep only the common columns (create if missing)\n",
    "    for c in common_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    df = df[common_cols].copy()\n",
    "    # coerce numeric\n",
    "    num_cols = [c for c in common_cols if c not in {\"City\", \"Assumed_building_type\"}]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # city as string\n",
    "    df[\"City\"] = df[\"City\"].astype(\"string\").str.strip()\n",
    "    # ensure building type consistent casing if string\n",
    "    if df[\"Assumed_building_type\"].dtype == \"O\" or pd.api.types.is_string_dtype(df[\"Assumed_building_type\"]):\n",
    "        df[\"Assumed_building_type\"] = df[\"Assumed_building_type\"].astype(\"string\").str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "dfs = {f.stem: read_standardize(f, common_columns) for f in files}\n",
    "print(\"Loaded and standardized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075aea7",
   "metadata": {},
   "source": [
    "## Step 1 — Show % of null rows **per dataset** (before we drop anything)\n",
    "We compute the percentage of rows that have **any null** within the selected `common_columns`. This lets you decide if dropping is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 2: Null % per dataset ===\n",
    "from pandas import DataFrame\n",
    "\n",
    "summary_rows = []\n",
    "for name, df in dfs.items():\n",
    "    n = len(df)\n",
    "    # rows with any null across the selected columns\n",
    "    any_null = df[common_columns].isna().any(axis=1).sum()\n",
    "    pct = (any_null / n * 100) if n else 0.0\n",
    "    summary_rows.append({\"dataset\": name, \"rows\": n, \"rows_with_any_null\": any_null, \"pct_with_any_null\": round(pct, 2)})\n",
    "\n",
    "null_summary = DataFrame(summary_rows).sort_values(\"pct_with_any_null\", ascending=False).reset_index(drop=True)\n",
    "null_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6854c9",
   "metadata": {},
   "source": [
    "## Step 2 — Label-encode `Assumed_building_type`\n",
    "Mapping used (stable across all files):\n",
    "\n",
    "- 0 → commercial\n",
    "- 1 → industrial\n",
    "- 2 → multifamily residential\n",
    "- 3 → public\n",
    "- 4 → single family residential\n",
    "\n",
    "Below we preview unique values per dataset **before** encoding, then encode to the mapping and show a quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 3: Inspect unique values then encode ===\n",
    "preview = {}\n",
    "for name, df in dfs.items():\n",
    "    u = df[\"Assumed_building_type\"].dropna().unique()\n",
    "    preview[name] = sorted(map(str, u.tolist())) if len(u) else []\n",
    "preview_df = pd.DataFrame.from_dict(preview, orient=\"index\")\n",
    "print(\"Unique 'Assumed_building_type' values BEFORE encoding (per dataset):\")\n",
    "display(preview_df)\n",
    "\n",
    "def encode_building_type(series: pd.Series) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    # if numeric already, keep (but ensure it's int-like and within 0..4)\n",
    "    if pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        # keep as pandas nullable Int64\n",
    "        s = s.where(s.isin(list(BT_INT2STR.keys())), other=pd.NA).astype(\"Int64\")\n",
    "        return s\n",
    "    # else map from string -> int\n",
    "    s = s.astype(\"string\").str.strip().str.lower().map(BT_STR2INT)\n",
    "    return s.astype(\"Int64\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    dfs[name][\"Assumed_building_type\"] = encode_building_type(df[\"Assumed_building_type\"])\n",
    "\n",
    "print(\"\\nMapping used (int -> name):\", BT_INT2STR)\n",
    "# quick post-check\n",
    "check = {name: sorted(dfs[name][\"Assumed_building_type\"].dropna().unique().tolist()) for name in dfs}\n",
    "print(\"Encoded unique codes per dataset:\", check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd535e3",
   "metadata": {},
   "source": [
    "## Step 3 — Drop rows with **any null** in the selected columns\n",
    "We first report **how many rows will be dropped** per dataset, then perform the drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a783298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 4: Report and drop null rows ===\n",
    "drop_report = []\n",
    "for name, df in dfs.items():\n",
    "    n0 = len(df)\n",
    "    mask_keep = ~df[common_columns].isna().any(axis=1)\n",
    "    dropped = int((~mask_keep).sum())\n",
    "    kept = int(mask_keep.sum())\n",
    "    drop_report.append({\"dataset\": name, \"before\": n0, \"drop_null_rows\": dropped, \"after\": kept, \"pct_dropped\": round((dropped/n0*100) if n0 else 0.0, 2)})\n",
    "    dfs[name] = df.loc[mask_keep].reset_index(drop=True)\n",
    "\n",
    "drop_null_df = pd.DataFrame(drop_report).sort_values(\"pct_dropped\", ascending=False).reset_index(drop=True)\n",
    "drop_null_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf71db",
   "metadata": {},
   "source": [
    "## Step 4 — Remove rows where `Estimated_building_height` is 0 or negative\n",
    "This step is applied **after** the null-row drop. We report how many rows are removed per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae870ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 5: Filter non-positive heights ===\n",
    "height_report = []\n",
    "for name, df in dfs.items():\n",
    "    n0 = len(df)\n",
    "    bad = df[\"Estimated_building_height\"].le(0).sum()\n",
    "    dfs[name] = df.loc[df[\"Estimated_building_height\"].gt(0)].reset_index(drop=True)\n",
    "    height_report.append({\"dataset\": name, \"before\": n0, \"removed_height_le_zero\": int(bad), \"after\": len(dfs[name]),\n",
    "                          \"pct_removed\": round((bad/n0*100) if n0 else 0.0, 2)})\n",
    "height_le0_df = pd.DataFrame(height_report).sort_values(\"pct_removed\", ascending=False).reset_index(drop=True)\n",
    "height_le0_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d29c5",
   "metadata": {},
   "source": [
    "## Step 5 — Save cleaned data (per-city + combined)\n",
    "We save each cleaned dataset to Parquet in `data/interim/cleaned/` and a single combined file `data/interim/all_cities_clean.parquet`. Change paths if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc83561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 6: Write outputs ===\n",
    "import polars as pl\n",
    "\n",
    "combined = []\n",
    "for name, df in dfs.items():\n",
    "    out_path = OUT_DIR / f\"{name}.parquet\"\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    combined.append(df.assign(filename=f\"{name}.parquet\"))\n",
    "    print(\"Wrote:\", out_path)\n",
    "\n",
    "combined_df = pd.concat(combined, ignore_index=True)\n",
    "combined_out = Path(\"data/interim/all_cities_clean.parquet\")\n",
    "combined_df.to_parquet(combined_out, index=False)\n",
    "print(\"Wrote combined:\", combined_out.resolve())\n",
    "\n",
    "# Quick sanity: basic shapes\n",
    "print(\"Per-dataset cleaned sizes:\")\n",
    "print({k: len(v) for k,v in dfs.items()})\n",
    "print(\"Combined:\", combined_df.shape)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
